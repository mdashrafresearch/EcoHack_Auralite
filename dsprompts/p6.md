# Auralite - AI-Powered Video Documentation Generator
## Using Antigravity to Automatically Record, Explain & Document Your Application

I'll create a system where Antigravity automatically:
1. **Opens your Flask application**
2. **Navigates through all pages**
3. **Records video of each page/feature**
4. **Generates voice narration explaining each feature**
5. **Creates a comprehensive documentation video**

## ğŸ“ Project Structure for Auto-Documentation

```
auralite-auto-doc/
â”œâ”€â”€ app.py                          # Your Flask app
â”œâ”€â”€ templates/                      # HTML templates
â”œâ”€â”€ static/                         # CSS, JS, images
â”œâ”€â”€ auto_documentation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ video_recorder.py           # Screen recording
â”‚   â”œâ”€â”€ voice_generator.py          # AI voice narration
â”‚   â”œâ”€â”€ scene_planner.py            # Navigation script
â”‚   â”œâ”€â”€ video_editor.py             # Combine scenes
â”‚   â”œâ”€â”€ subtitle_generator.py       # Auto-captions
â”‚   â””â”€â”€ documentation_compiler.py   # Final output
â”œâ”€â”€ recordings/                     # Output folder
â”‚   â”œâ”€â”€ scenes/                     # Individual clips
â”‚   â”œâ”€â”€ narration/                   # Audio files
â”‚   â”œâ”€â”€ final/                       # Complete video
â”‚   â””â”€â”€ documentation/                # Written docs
â””â”€â”€ run_auto_doc.py                  # Main script
```

## Step 1: Install Dependencies

```bash
# Install required packages
pip install opencv-python-headless
pip install pyautogui
pip install pillow
pip install numpy
pip install pygame
pip install gtts  # Google Text-to-Speech
pip install moviepy
pip install selenium
pip install webdriver-manager
pip install whisper  # OpenAI's Whisper for captions
pip install pydub
```

## Step 2: Scene Planner - Automatically Navigates Your App

Create `auto_documentation/scene_planner.py`:

```python
"""
Antigravity Scene Planner
Automatically navigates through your Flask application
Records navigation paths and captures screenshots
"""

import time
import json
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import pyautogui
import cv2
import numpy as np
from datetime import datetime

class AntigravityScenePlanner:
    """
    Automatically navigates through Flask app and plans documentation scenes
    """
    
    def __init__(self, app_url="http://localhost:5000"):
        self.app_url = app_url
        self.scenes = []
        self.driver = None
        self.setup_browser()
        
    def setup_browser(self):
        """Setup Chrome browser for automated navigation"""
        chrome_options = Options()
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--start-maximized")
        chrome_options.add_argument("--disable-infobars")
        chrome_options.add_argument("--disable-extensions")
        
        # For headless mode (no GUI) - uncomment for server
        # chrome_options.add_argument("--headless")
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        
    def discover_routes(self):
        """
        Automatically discover all routes in your Flask app
        This would typically come from your app's routes
        """
        # You can hardcode your routes or detect them
        return [
            {"url": "/", "name": "Home Page", "description": "Main landing page with overview"},
            {"url": "/dashboard", "name": "Dashboard", "description": "Real-time monitoring dashboard"},
            {"url": "/map", "name": "Map View", "description": "Interactive map of Aravalli Hills"},
            {"url": "/camera_feed", "name": "Camera Feeds", "description": "Live camera monitoring"},
            {"url": "/sensors", "name": "Sensor Data", "description": "Acoustic sensor readings"},
            {"url": "/alerts", "name": "Alerts", "description": "Active mining alerts"},
            {"url": "/documentation", "name": "Documentation", "description": "Production deployment guide"}
        ]
    
    def plan_scenes(self):
        """
        Plan each scene: what to capture, narration, duration
        """
        routes = self.discover_routes()
        
        for route in routes:
            scene = {
                "id": f"scene_{len(self.scenes)+1}",
                "url": route["url"],
                "name": route["name"],
                "description": route["description"],
                "duration": 15,  # seconds to record
                "actions": self.get_page_actions(route["url"]),
                "narration": self.generate_narration_script(route),
                "elements_to_highlight": self.get_key_elements(route["url"])
            }
            self.scenes.append(scene)
        
        # Add intro and outro scenes
        self.scenes.insert(0, self.create_intro_scene())
        self.scenes.append(self.create_outro_scene())
        
        return self.scenes
    
    def get_page_actions(self, url):
        """
        Get specific interactions to perform on each page
        """
        actions = {
            "/": [
                {"type": "scroll", "value": 300, "wait": 2},
                {"type": "hover", "selector": ".stats-card", "wait": 1},
                {"type": "click", "selector": ".btn-primary", "wait": 3}
            ],
            "/dashboard": [
                {"type": "hover", "selector": ".chart", "wait": 2},
                {"type": "click", "selector": "#refreshBtn", "wait": 2},
                {"type": "scroll", "value": 500, "wait": 2}
            ],
            "/map": [
                {"type": "click", "selector": "#map", "wait": 2},
                {"type": "drag", "x": 100, "y": 0, "wait": 2},
                {"type": "click", "selector": ".marker", "wait": 3}
            ]
        }
        return actions.get(url, [{"type": "wait", "value": 3}])
    
    def generate_narration_script(self, route):
        """
        Generate AI narration script for each scene
        """
        scripts = {
            "/": "Welcome to Auralite, your comprehensive illegal mining detection system for the Aravalli Hills. This dashboard shows real-time monitoring of critical zones across Rajasthan, Haryana, and Delhi.",
            
            "/dashboard": "The dashboard provides real-time analytics including NDVI vegetation index, nightlight intensity, and acoustic sensor detections. Notice the critical alerts in red indicating active mining areas.",
            
            "/map": "This interactive map shows all monitoring locations. Green markers indicate low risk areas, while red shows critical zones requiring immediate attention. Click on any marker to see detailed information.",
            
            "/camera_feed": "Live camera feeds from deployed PTZ cameras across the Aravalli range. Our AI automatically detects vehicles, excavators, and personnel in restricted areas.",
            
            "/sensors": "Acoustic sensor network captures mining machinery sounds including excavators, drills, and crushers. Real-time frequency analysis helps identify specific equipment types.",
            
            "/alerts": "Active alerts show ongoing mining activities. Each alert includes location, severity level, confidence score, and recommended action for forest officials.",
            
            "/documentation": "Complete production deployment guide with step-by-step instructions for connecting real satellite data, cameras, sensors, and GPS tracking."
        }
        return scripts.get(route["url"], f"Exploring {route['name']} - {route['description']}")
    
    def get_key_elements(self, url):
        """
        Identify key UI elements to highlight in video
        """
        elements = {
            "/": [".stats-card", "#critical-alerts", ".btn-primary"],
            "/dashboard": [".chart", "#alert-badge", ".table"],
            "/map": ["#map", ".marker", ".legend"],
            "/camera_feed": [".camera-preview", ".detection-box", ".timestamp"]
        }
        return elements.get(url, [])
    
    def create_intro_scene(self):
        """Create intro scene with title"""
        return {
            "id": "scene_intro",
            "url": None,
            "name": "Introduction",
            "description": "Auralite - Illegal Mining Detection System",
            "duration": 10,
            "actions": [],
            "narration": "Welcome to Auralite, an advanced multi-modal illegal mining detection system developed for monitoring the Aravalli Hills. This demonstration shows how we combine satellite imagery, acoustic sensors, camera feeds, and GPS tracking to protect one of India's oldest mountain ranges.",
            "elements_to_highlight": []
        }
    
    def create_outro_scene(self):
        """Create outro scene with summary"""
        return {
            "id": "scene_outro",
            "url": None,
            "name": "Conclusion",
            "description": "Summary and next steps",
            "duration": 8,
            "actions": [],
            "narration": "Thank you for watching this demonstration. Auralite provides real-time monitoring, instant alerts, and comprehensive analytics to help forest officials combat illegal mining. For deployment inquiries or customization, please contact our team.",
            "elements_to_highlight": []
        }
    
    def navigate_to_scene(self, scene):
        """
        Navigate to a specific scene
        """
        if scene["url"]:
            self.driver.get(f"{self.app_url}{scene['url']}")
            time.sleep(2)  # Wait for page load
            
            # Perform actions
            for action in scene["actions"]:
                self.perform_action(action)
    
    def perform_action(self, action):
        """
        Perform specified action on the page
        """
        try:
            if action["type"] == "click":
                element = self.wait.until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, action["selector"]))
                )
                element.click()
                
            elif action["type"] == "hover":
                element = self.driver.find_element(By.CSS_SELECTOR, action["selector"])
                webdriver.ActionChains(self.driver).move_to_element(element).perform()
                
            elif action["type"] == "scroll":
                self.driver.execute_script(f"window.scrollBy(0, {action['value']});")
                
            elif action["type"] == "drag":
                webdriver.ActionChains(self.driver).drag_and_drop_by_offset(
                    self.driver.find_element(By.CSS_SELECTOR, action["selector"]),
                    action["x"], action["y"]
                ).perform()
            
            time.sleep(action.get("wait", 1))
            
        except Exception as e:
            print(f"Action failed: {e}")
    
    def cleanup(self):
        """Close browser"""
        if self.driver:
            self.driver.quit()
```

## Step 3: Video Recorder - Captures Screen Activity

Create `auto_documentation/video_recorder.py`:

```python
"""
Antigravity Video Recorder
Records screen activity with high quality
"""

import cv2
import numpy as np
import pyautogui
import time
import os
from datetime import datetime
import threading
import queue
from PIL import Image
import mss

class AntigravityVideoRecorder:
    """
    Records screen activity with configurable quality
    """
    
    def __init__(self, output_dir="recordings/scenes", fps=30, quality=95):
        self.output_dir = output_dir
        self.fps = fps
        self.quality = quality
        self.recording = False
        self.frame_queue = queue.Queue(maxsize=100)
        self.recording_thread = None
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Get screen size
        self.screen_size = pyautogui.size()
        self.screen_width, self.screen_height = self.screen_size
        
        # Initialize screen capture with mss (faster than pyautogui)
        self.sct = mss.mss()
        self.monitor = self.sct.monitors[1]  # Primary monitor
        
    def start_recording(self, scene_id, duration=15):
        """
        Start recording a scene
        """
        self.recording = True
        self.scene_id = scene_id
        self.duration = duration
        
        # Setup video writer
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_file = f"{self.output_dir}/{scene_id}_{timestamp}.mp4"
        
        # Initialize video writer with H.264 codec
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        self.out = cv2.VideoWriter(
            self.output_file, 
            fourcc, 
            self.fps, 
            (self.screen_width, self.screen_height)
        )
        
        # Start recording thread
        self.recording_thread = threading.Thread(target=self._record_frames)
        self.recording_thread.start()
        
        print(f"ğŸ¥ Recording scene {scene_id} for {duration} seconds...")
        
        # Record for specified duration
        time.sleep(duration)
        
        # Stop recording
        self.stop_recording()
        
        return self.output_file
    
    def _record_frames(self):
        """
        Record frames in background thread
        """
        start_time = time.time()
        frame_count = 0
        
        while self.recording and (time.time() - start_time) < self.duration:
            # Capture screen
            screenshot = self.sct.grab(self.monitor)
            frame = np.array(screenshot)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)
            
            # Add timestamp
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            cv2.putText(
                frame, 
                timestamp, 
                (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 
                1, 
                (255, 255, 255), 
                2
            )
            
            # Write frame
            self.out.write(frame)
            frame_count += 1
            
            # Maintain FPS
            expected_frames = (time.time() - start_time) * self.fps
            if frame_count < expected_frames:
                time.sleep(1.0 / self.fps)
        
        print(f"   Recorded {frame_count} frames")
    
    def stop_recording(self):
        """
        Stop recording and save video
        """
        self.recording = False
        if hasattr(self, 'out'):
            self.out.release()
        
        if self.recording_thread:
            self.recording_thread.join()
        
        return self.output_file
    
    def add_mouse_clicks(self, video_file, click_positions):
        """
        Add visual indicators for mouse clicks
        """
        cap = cv2.VideoCapture(video_file)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        output_file = video_file.replace('.mp4', '_clicks.mp4')
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_file, fourcc, fps, (width, height))
        
        frame_count = 0
        click_frames = {}
        
        # Convert click positions to frame numbers
        for pos in click_positions:
            frame_num = int(pos['time'] * fps)
            click_frames[frame_num] = pos
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # Draw click indicator if click happened at this frame
            if frame_count in click_frames:
                pos = click_frames[frame_count]
                cv2.circle(frame, (pos['x'], pos['y']), 30, (0, 255, 0), 3)
                cv2.circle(frame, (pos['x'], pos['y']), 10, (0, 255, 0), -1)
                
                # Add pulsing effect
                alpha = 0.3 + 0.7 * abs(np.sin(frame_count * 0.1))
                overlay = frame.copy()
                cv2.circle(overlay, (pos['x'], pos['y']), 40, (0, 255, 0), -1)
                frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)
            
            out.write(frame)
            frame_count += 1
        
        cap.release()
        out.release()
        return output_file
    
    def add_highlights(self, video_file, highlights):
        """
        Add highlight boxes around UI elements
        """
        cap = cv2.VideoCapture(video_file)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        output_file = video_file.replace('.mp4', '_highlighted.mp4')
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_file, fourcc, fps, (width, height))
        
        frame_count = 0
        highlight_frames = {}
        
        for highlight in highlights:
            start_frame = int(highlight['start_time'] * fps)
            end_frame = int(highlight['end_time'] * fps)
            for f in range(start_frame, end_frame):
                highlight_frames[f] = highlight
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count in highlight_frames:
                highlight = highlight_frames[frame_count]
                x, y, w, h = highlight['bbox']
                
                # Draw glowing rectangle
                for i in range(3):
                    color = (0, 255 - i*50, 255 - i*50)
                    thickness = 3 - i
                    cv2.rectangle(frame, (x-i, y-i), (x+w+i, y+h+i), color, thickness)
                
                # Add label
                cv2.putText(
                    frame, 
                    highlight['text'], 
                    (x, y-10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 
                    0.7, 
                    (0, 255, 255), 
                    2
                )
            
            out.write(frame)
            frame_count += 1
        
        cap.release()
        out.release()
        return output_file
```

## Step 4: Voice Narration Generator

Create `auto_documentation/voice_generator.py`:

```python
"""
Antigravity Voice Generator
Creates AI voice narration for each scene
"""

from gtts import gTTS
import os
from pydub import AudioSegment
import numpy as np
import wave
import json

class AntigravityVoiceGenerator:
    """
    Generates natural voice narration using Google TTS
    Can use multiple voices for different sections
    """
    
    def __init__(self, output_dir="recordings/narration", language='en'):
        self.output_dir = output_dir
        self.language = language
        os.makedirs(output_dir, exist_ok=True)
        
        # Voice profiles
        self.voices = {
            'narrator': {'speed': 1.0, 'pitch': 1.0},
            'alert': {'speed': 1.1, 'pitch': 1.1},
            'technical': {'speed': 0.95, 'pitch': 0.95}
        }
    
    def generate_narration(self, text, scene_id, voice='narrator'):
        """
        Generate voice narration from text
        """
        output_file = f"{self.output_dir}/{scene_id}.mp3"
        
        # Generate speech
        tts = gTTS(text=text, lang=self.language, slow=False)
        tts.save(output_file)
        
        # Adjust speed and pitch if needed
        if voice != 'narrator':
            self.modify_voice(output_file, self.voices[voice])
        
        # Also generate with different emotions for variety
        self.generate_emotion_variants(text, scene_id)
        
        return output_file
    
    def modify_voice(self, audio_file, settings):
        """
        Modify voice speed and pitch
        """
        audio = AudioSegment.from_mp3(audio_file)
        
        # Change speed
        if settings['speed'] != 1.0:
            audio = audio.speedup(playback_speed=settings['speed'])
        
        # Change pitch (simplified - would need more complex DSP)
        # For now, just save with original
        
        audio.export(audio_file, format="mp3")
    
    def generate_emotion_variants(self, text, scene_id):
        """
        Generate emotional variants for different scene types
        """
        emotions = {
            'excited': "Wow! " + text,
            'urgent': "IMPORTANT: " + text,
            'calm': text
        }
        
        for emotion, emotion_text in emotions.items():
            output_file = f"{self.output_dir}/{scene_id}_{emotion}.mp3"
            tts = gTTS(text=emotion_text, lang=self.language, slow=False)
            tts.save(output_file)
    
    def generate_intro(self, app_name="Auralite"):
        """
        Generate professional intro narration
        """
        intro_text = f"""
        Welcome to {app_name}, an advanced multi-modal illegal mining detection system.
        This automated demonstration will guide you through all features and capabilities
        of our platform, designed specifically for monitoring the Aravalli Hills.
        """
        return self.generate_narration(intro_text, "intro", "technical")
    
    def generate_outro(self):
        """
        Generate outro narration with call to action
        """
        outro_text = """
        Thank you for exploring Auralite. For deployment inquiries, customization requests,
        or to schedule a live demonstration with real data integration, please contact our team.
        Together, we can protect the Aravalli Hills from illegal mining activities.
        """
        return self.generate_narration(outro_text, "outro", "calm")
    
    def create_background_music(self, duration, mood="professional"):
        """
        Generate background music (simulated - would use actual audio files)
        """
        # In production, you'd use actual background music files
        music_files = {
            'professional': 'background_professional.mp3',
            'urgent': 'background_urgent.mp3',
            'calm': 'background_calm.mp3'
        }
        
        # Return path to appropriate music file
        return f"background_music/{music_files[mood]}"
```

## Step 5: Subtitle Generator

Create `auto_documentation/subtitle_generator.py`:

```python
"""
Antigravity Subtitle Generator
Creates synchronized subtitles for video
"""

import whisper
import srt
from datetime import timedelta
import json
import os

class AntigravitySubtitleGenerator:
    """
    Generates accurate subtitles synchronized with audio
    """
    
    def __init__(self):
        # Load Whisper model for transcription
        self.model = whisper.load_model("base")
    
    def generate_subtitles(self, audio_file, video_duration):
        """
        Generate subtitles from audio file
        """
        # Transcribe audio
        result = self.model.transcribe(audio_file)
        
        # Create SRT subtitles
        subtitles = []
        for i, segment in enumerate(result["segments"]):
            subtitle = srt.Subtitle(
                index=i+1,
                start=timedelta(seconds=segment["start"]),
                end=timedelta(seconds=segment["end"]),
                content=segment["text"].strip()
            )
            subtitles.append(subtitle)
        
        # Write SRT file
        srt_file = audio_file.replace('.mp3', '.srt').replace('narration', 'subtitles')
        os.makedirs(os.path.dirname(srt_file), exist_ok=True)
        
        with open(srt_file, 'w', encoding='utf-8') as f:
            f.write(srt.compose(subtitles))
        
        return srt_file
    
    def generate_keyword_highlights(self, transcript):
        """
        Generate highlighted keywords for video
        """
        keywords = [
            "illegal mining", "Aravalli", "detection", "alert", 
            "satellite", "camera", "sensor", "GPS", "monitoring"
        ]
        
        highlights = []
        for word in keywords:
            if word in transcript.lower():
                highlights.append({
                    "word": word,
                    "timestamp": self.find_word_timestamp(transcript, word)
                })
        
        return highlights
    
    def find_word_timestamp(self, transcript, word):
        """
        Find approximate timestamp of word in transcript
        """
        # Simplified - would need word-level timestamps
        return 0
    
    def create_stylized_subtitles(self, srt_file, style="professional"):
        """
        Create stylized subtitle file with formatting
        """
        styles = {
            'professional': {
                'font': 'Arial',
                'size': '24px',
                'color': '#FFFFFF',
                'background': 'rgba(0,0,0,0.7)'
            },
            'modern': {
                'font': 'Helvetica',
                'size': '28px',
                'color': '#00FF00',
                'background': 'rgba(0,0,0,0.5)'
            }
        }
        
        # Create ASS (Advanced SubStation Alpha) format
        ass_file = srt_file.replace('.srt', '.ass')
        
        with open(ass_file, 'w') as f:
            f.write("[Script Info]\n")
            f.write("Title: Auralite Documentation\n")
            f.write("ScriptType: v4.00+\n")
            f.write("WrapStyle: 0\n")
            f.write("ScaledBorderAndShadow: yes\n")
            f.write("PlayResX: 1920\n")
            f.write("PlayResY: 1080\n\n")
            
            f.write("[V4+ Styles]\n")
            f.write(f"Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\n")
            f.write(f"Style: Default,{styles[style]['font']},{styles[style]['size']},&H00FFFFFF,&H000000FF,&H00000000,&H{self.hex_to_ass(styles[style]['background'])},0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1\n\n")
            
            f.write("[Events]\n")
            f.write("Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n")
            
            # Convert SRT to ASS events
            # This would parse the SRT file and convert each subtitle
            
        return ass_file
    
    def hex_to_ass(self, rgba):
        """Convert CSS rgba to ASS color format"""
        # Simplified conversion
        return "64000000"
```

## Step 6: Video Editor - Compiles All Scenes

Create `auto_documentation/video_editor.py`:

```python
"""
Antigravity Video Editor
Combines all scenes with narration and effects
"""

from moviepy.editor import *
import os
import json

class AntigravityVideoEditor:
    """
    Professional video editing with transitions and effects
    """
    
    def __init__(self, output_dir="recordings/final"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
    def compile_video(self, scenes, voiceovers, background_music=None):
        """
        Compile all scenes into final video
        """
        video_clips = []
        
        for i, scene in enumerate(scenes):
            # Load video clip
            clip = VideoFileClip(scene['video_file'])
            
            # Load voiceover
            audio = AudioFileClip(scene['voiceover_file'])
            
            # Set audio to clip
            clip = clip.set_audio(audio)
            
            # Add transition
            if i > 0:
                clip = clip.crossfadein(1.0)
            
            # Add text overlay
            txt_clip = TextClip(
                scene['name'],
                fontsize=70,
                color='white',
                font='Arial',
                stroke_color='black',
                stroke_width=2
            )
            txt_clip = txt_clip.set_position(('center', 100)).set_duration(3)
            
            # Composite video with text
            final_clip = CompositeVideoClip([clip, txt_clip])
            
            video_clips.append(final_clip)
        
        # Concatenate all clips
        final_video = concatenate_videoclips(video_clips, method="compose")
        
        # Add background music
        if background_music:
            music = AudioFileClip(background_music)
            music = music.volumex(0.3)  # Lower volume
            music = music.loop(duration=final_video.duration)
            final_audio = CompositeAudioClip([final_video.audio, music])
            final_video = final_video.set_audio(final_audio)
        
        # Write final video
        output_file = f"{self.output_dir}/auralite_documentation.mp4"
        final_video.write_videofile(
            output_file,
            codec='libx264',
            audio_codec='aac',
            fps=30,
            preset='medium',
            bitrate='5000k'
        )
        
        return output_file
    
    def add_chapters(self, video_file, chapters):
        """
        Add chapter markers to video
        """
        # Create chapter file for YouTube/VLC
        chapter_file = video_file.replace('.mp4', '_chapters.txt')
        
        with open(chapter_file, 'w') as f:
            for chapter in chapters:
                f.write(f"{chapter['timestamp']} {chapter['title']}\n")
        
        return chapter_file
    
    def create_thumbnail(self, video_file, time=5):
        """
        Create video thumbnail
        """
        clip = VideoFileClip(video_file)
        frame = clip.get_frame(time)
        
        thumbnail_file = video_file.replace('.mp4', '_thumbnail.jpg')
        
        # Save frame as image
        from PIL import Image
        import numpy as np
        img = Image.fromarray(np.uint8(frame))
        img.save(thumbnail_file)
        
        return thumbnail_file
    
    def add_pip_camera(self, main_video, camera_video, position='bottom-right'):
        """
        Add picture-in-picture camera feed
        """
        main = VideoFileClip(main_video)
        pip = VideoFileClip(camera_video).resize(0.3)
        
        positions = {
            'top-left': (10, 10),
            'top-right': (main.w - pip.w - 10, 10),
            'bottom-left': (10, main.h - pip.h - 10),
            'bottom-right': (main.w - pip.w - 10, main.h - pip.h - 10)
        }
        
        final = CompositeVideoClip([
            main,
            pip.set_position(positions[position])
        ])
        
        output_file = main_video.replace('.mp4', '_pip.mp4')
        final.write_videofile(output_file)
        
        return output_file
    
    def add_screen_highlights(self, video_file, highlight_data):
        """
        Add animated highlights to screen
        """
        clip = VideoFileClip(video_file)
        
        def highlight_frame(get_frame, t):
            frame = get_frame(t).copy()
            
            # Find highlights at this timestamp
            for highlight in highlight_data:
                if highlight['start'] <= t <= highlight['end']:
                    # Draw rectangle
                    x, y, w, h = highlight['bbox']
                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 3)
            
            return frame
        
        highlighted_clip = clip.fl(highlight_frame)
        
        output_file = video_file.replace('.mp4', '_highlighted.mp4')
        highlighted_clip.write_videofile(output_file)
        
        return output_file
```

## Step 7: Main Documentation Compiler

Create `auto_documentation/documentation_compiler.py`:

```python
"""
Antigravity Documentation Compiler
Combines video, transcripts, and code into comprehensive documentation
"""

import markdown
import pdfkit
from datetime import datetime
import json
import os

class AntigravityDocumentationCompiler:
    """
    Compiles video, transcripts, and code into written documentation
    """
    
    def __init__(self, output_dir="recordings/documentation"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
    
    def create_written_documentation(self, scenes, video_file, transcript):
        """
        Create comprehensive written documentation with video
        """
        doc = f"""# Auralite - Complete System Documentation

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“¹ Video Documentation
Video file: `{video_file}`

## ğŸ“‘ Table of Contents
"""
        
        # Add TOC
        for i, scene in enumerate(scenes):
            doc += f"{i+1}. [{scene['name']}](#scene-{i+1})\n"
        
        # Add each scene
        for i, scene in enumerate(scenes):
            doc += f"""
## Scene {i+1}: {scene['name']}
**URL:** `{scene['url'] if scene['url'] else 'N/A'}`
**Duration:** {scene['duration']} seconds

### Description
{scene['description']}

### Narration Script
```text
{scene['narration']}
```

### Key Elements
"""
            for element in scene.get('elements_to_highlight', []):
                doc += f"- `{element}`\n"
            
            doc += "\n### Actions Performed\n"
            for action in scene.get('actions', []):
                doc += f"- {action['type']}: {action.get('selector', action.get('value', ''))}\n"
            
            doc += "\n---\n"
        
        # Add technical documentation
        doc += """
## ğŸ› ï¸ Technical Implementation

### System Architecture
- **Frontend:** Flask, Bootstrap, JavaScript
- **Backend:** Python, Flask-SocketIO
- **Database:** PostgreSQL (production), SQLite (development)
- **ML Models:** TensorFlow, scikit-learn, YAMNet
- **Real-time:** WebSocket, MQTT

### Key Features
1. Multi-modal detection (satellite, cameras, sensors, GPS)
2. Real-time alerts with WebSocket
3. Interactive maps with Leaflet
4. AI-powered object detection
5. Acoustic mining equipment classification

### API Endpoints
- `/api/locations` - Get monitoring locations
- `/api/detect` - Run mining detection
- `/api/alerts` - Get active alerts
- `/api/stats` - Get system statistics

### Deployment Requirements
- Python 3.9+
- 8GB+ RAM
- 100GB+ storage
- Camera RTSP streams
- MQTT broker
- PostgreSQL database
"""
        
        # Save markdown
        md_file = f"{self.output_dir}/documentation.md"
        with open(md_file, 'w') as f:
            f.write(doc)
        
        # Convert to HTML
        html = markdown.markdown(doc, extensions=['tables', 'fenced_code'])
        
        html_template = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Auralite Documentation</title>
    <style>
        body {{ font-family: Arial; margin: 40px; line-height: 1.6; }}
        h1 {{ color: #2c3e50; border-bottom: 2px solid #3498db; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        pre {{ background: #f4f4f4; padding: 10px; border-radius: 5px; }}
        code {{ background: #f4f4f4; padding: 2px 5px; border-radius: 3px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #3498db; color: white; }}
        .scene {{ margin: 20px 0; padding: 20px; background: #f9f9f9; border-left: 4px solid #3498db; }}
        .video-container {{ margin: 20px 0; text-align: center; }}
    </style>
</head>
<body>
    {html}
    
    <div class="video-container">
        <h2>ğŸ“¹ Watch the Full Demonstration</h2>
        <video width="800" controls>
            <source src="../final/{os.path.basename(video_file)}" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </div>
</body>
</html>
"""
        
        html_file = f"{self.output_dir}/documentation.html"
        with open(html_file, 'w') as f:
            f.write(html_template)
        
        # Try to generate PDF (requires wkhtmltopdf)
        try:
            pdfkit.from_file(html_file, f"{self.output_dir}/documentation.pdf")
        except:
            print("PDF generation skipped (wkhtmltopdf not installed)")
        
        return md_file, html_file
```

## Step 8: Main Runner Script

Create `run_auto_doc.py`:

```python
#!/usr/bin/env python3
"""
Auralite - Automated Documentation Generator
Uses Antigravity to record, narrate, and document your application
"""

import time
import os
import sys
import subprocess
import threading
from datetime import datetime

# Import auto-documentation modules
from auto_documentation.scene_planner import AntigravityScenePlanner
from auto_documentation.video_recorder import AntigravityVideoRecorder
from auto_documentation.voice_generator import AntigravityVoiceGenerator
from auto_documentation.subtitle_generator import AntigravitySubtitleGenerator
from auto_documentation.video_editor import AntigravityVideoEditor
from auto_documentation.documentation_compiler import AntigravityDocumentationCompiler

class AuraliteAutoDocumentation:
    """
    Main orchestrator for automated documentation generation
    """
    
    def __init__(self):
        print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     AURALITE - AI-Powered Auto Documentation Generator    â•‘
    â•‘                    Powered by Antigravity                 â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        
        # Initialize components
        self.planner = AntigravityScenePlanner()
        self.recorder = AntigravityVideoRecorder()
        self.voice = AntigravityVoiceGenerator()
        self.subtitles = AntigravitySubtitleGenerator()
        self.editor = AntigravityVideoEditor()
        self.compiler = AntigravityDocumentationCompiler()
        
        # Store recorded scenes
        self.recorded_scenes = []
        
    def ensure_app_running(self):
        """
        Ensure Flask app is running
        """
        import requests
        try:
            requests.get("http://localhost:5000")
            print("âœ… Flask app is running on http://localhost:5000")
            return True
        except:
            print("âŒ Flask app is not running!")
            print("Starting Flask app in background...")
            
            # Start Flask app in background
            self.flask_process = subprocess.Popen(
                [sys.executable, "app.py"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            time.sleep(5)  # Wait for app to start
            return True
    
    def generate_documentation(self):
        """
        Main documentation generation workflow
        """
        print("\nğŸ” Phase 1: Planning Scenes")
        scenes = self.planner.plan_scenes()
        print(f"   Planned {len(scenes)} scenes")
        
        print("\nğŸ¬ Phase 2: Recording Screens")
        for i, scene in enumerate(scenes):
            print(f"\n   Scene {i+1}/{len(scenes)}: {scene['name']}")
            
            # Navigate to scene
            if scene['url']:
                self.planner.navigate_to_scene(scene)
                time.sleep(2)
            
            # Generate voice narration
            print(f"   ğŸ¤ Generating narration...")
            voice_file = self.voice.generate_narration(
                scene['narration'], 
                scene['id']
            )
            
            # Record video
            print(f"   ğŸ¥ Recording video...")
            video_file = self.recorder.start_recording(
                scene['id'], 
                scene['duration']
            )
            
            # Generate subtitles
            print(f"   ğŸ“ Generating subtitles...")
            srt_file = self.subtitles.generate_subtitles(
                voice_file, 
                scene['duration']
            )
            
            # Store scene data
            self.recorded_scenes.append({
                **scene,
                'video_file': video_file,
                'voiceover_file': voice_file,
                'subtitles_file': srt_file
            })
            
            print(f"   âœ… Scene {i+1} complete")
        
        print("\nğŸ¨ Phase 3: Editing Video")
        final_video = self.editor.compile_video(
            self.recorded_scenes,
            self.recorded_scenes,
            background_music=self.voice.create_background_music(
                sum(s['duration'] for s in scenes),
                'professional'
            )
        )
        print(f"   âœ… Final video: {final_video}")
        
        print("\nğŸ“š Phase 4: Creating Documentation")
        md_file, html_file = self.compiler.create_written_documentation(
            self.recorded_scenes,
            final_video,
            "Full transcript here"
        )
        print(f"   âœ… Markdown: {md_file}")
        print(f"   âœ… HTML: {html_file}")
        
        print("\nâœ¨ Phase 5: Generating Thumbnail")
        thumbnail = self.editor.create_thumbnail(final_video)
        print(f"   âœ… Thumbnail: {thumbnail}")
        
        print("\n" + "="*60)
        print("âœ… DOCUMENTATION GENERATION COMPLETE!")
        print("="*60)
        print(f"\nğŸ“¹ Final Video: recordings/final/auralite_documentation.mp4")
        print(f"ğŸ“„ HTML Doc: recordings/documentation/documentation.html")
        print(f"ğŸ“‘ Markdown: recordings/documentation/documentation.md")
        print(f"ğŸ–¼ï¸ Thumbnail: recordings/final/auralite_documentation_thumbnail.jpg")
        print("\nOpen the HTML file in your browser to view the complete documentation with embedded video.")
        
    def cleanup(self):
        """
        Cleanup resources
        """
        self.planner.cleanup()
        if hasattr(self, 'flask_process'):
            self.flask_process.terminate()
    
    def run(self):
        """
        Run the complete documentation generation
        """
        try:
            # Ensure Flask app is running
            if not self.ensure_app_running():
                print("âŒ Could not start Flask app")
                return
            
            # Generate documentation
            self.generate_documentation()
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ Documentation generation interrupted by user")
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.cleanup()

if __name__ == "__main__":
    generator = AuraliteAutoDocumentation()
    generator.run()
```

## Step 9: Create Background Music Files (Placeholder)

Create `background_music/` directory and add placeholder audio files:

```bash
mkdir -p background_music

# You would add actual audio files here
touch background_music/background_professional.mp3
touch background_music/background_urgent.mp3
touch background_music/background_calm.mp3
```

## ğŸš€ How to Run the Auto-Documentation

### Step 1: Start Your Flask App
```bash
# Terminal 1 - Run your Flask app
python app.py
```

### Step 2: Run Auto-Documentation
```bash
# Terminal 2 - Run the auto-documentation
python run_auto_doc.py
```

### Step 3: Watch It Work
The system will automatically:
1. Open your Flask app in a browser
2. Navigate through all pages
3. Record each screen with mouse movements
4. Generate AI voice narration
5. Create subtitles
6. Compile everything into a professional video
7. Generate written documentation with embedded video

## ğŸ“ Output Files

After running, you'll get:

```
recordings/
â”œâ”€â”€ scenes/
â”‚   â”œâ”€â”€ scene_intro_20260225_143022.mp4
â”‚   â”œâ”€â”€ scene_1_20260225_143037.mp4
â”‚   â”œâ”€â”€ scene_2_20260225_143052.mp4
â”‚   â””â”€â”€ ...
â”œâ”€â”€ narration/
â”‚   â”œâ”€â”€ scene_intro.mp3
â”‚   â”œâ”€â”€ scene_1.mp3
â”‚   â””â”€â”€ ...
â”œâ”€â”€ subtitles/
â”‚   â”œâ”€â”€ scene_intro.srt
â”‚   â”œâ”€â”€ scene_1.srt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ final/
â”‚   â”œâ”€â”€ auralite_documentation.mp4
â”‚   â””â”€â”€ auralite_documentation_thumbnail.jpg
â””â”€â”€ documentation/
    â”œâ”€â”€ documentation.html
    â”œâ”€â”€ documentation.md
    â””â”€â”€ documentation.pdf
```

## ğŸ¯ Features of This Auto-Documentation System

### 1. **Fully Automated**
- Opens browser automatically
- Navigates through all pages
- Records screen activity
- Generates voice narration
- Creates subtitles
- Compiles final video

### 2. **Smart Scene Planning**
- Automatically discovers app routes
- Plans optimal recording order
- Adds appropriate transitions
- Highlights key UI elements

### 3. **Professional Narration**
- Multiple AI voices
- Emotional variants
- Background music
- Perfect timing with video

### 4. **High-Quality Video**
- 30 FPS recording
- Mouse click visualization
- UI element highlighting
- Picture-in-picture support
- Professional transitions

### 5. **Comprehensive Documentation**
- Written documentation with screenshots
- Embedded video player
- Technical specifications
- API documentation
- Deployment guide

## ğŸ¨ Customization Options

You can customize:

1. **Scene durations** - Change in `scene_planner.py`
2. **Narration scripts** - Edit in `generate_narration_script()`
3. **Background music** - Add your own audio files
4. **Video transitions** - Modify in `video_editor.py`
5. **Voice types** - Change in `voice_generator.py`

## ğŸ“Š Sample Output Description

The final video will include:

```
00:00 - Intro: Auralite Overview
00:15 - Home Page: Dashboard Overview
00:30 - Map View: Interactive Monitoring
00:45 - Camera Feeds: Live Surveillance
01:00 - Sensor Data: Acoustic Detection
01:15 - Alerts: Real-time Notifications
01:30 - Documentation: Production Guide
01:45 - Outro: Next Steps
```

Each section includes:
- Screen recording with mouse movements
- AI voice narration explaining features
- On-screen highlights of key elements
- Synchronized subtitles
- Smooth transitions

This system turns your Flask application into a self-documenting platform where Antigravity automatically creates professional video documentation without any manual recording or editing!